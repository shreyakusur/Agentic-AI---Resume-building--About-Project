# -*- coding: utf-8 -*-
"""Agentic AI - Resume Building - About Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mh9L0JnyAY3foVLFGlXZUL8F8ZDuID48

## Resume Building - Agentic Ai Framework - About project
"""

import os
from IPython.display import Markdown, display
from openai import OpenAI
import openai
import json

import getpass
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter key: ")

import getpass
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter key: ")

GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai"
gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=os.environ["GOOGLE_API_KEY"])

"""## About-project Agent"""

LinkedIN_about_project_rules = '''
I worked on a deep learning project using the Fashion-MNIST dataset to build and compare Artificial Neural Networks (ANN) and Convolutional Neural Networks (CNN) for image classification.
The project involved experimenting with multiple architectural and training settings to understand their effect on model performance.
*Implemented ANN and CNN models from scratch for fashion image recognition.
*Experimented with different activation functions (Sigmoid, Tanh, ReLU) to compare convergence behavior.
*Tuned network depth and hidden-layer neuron counts to analyze model complexity vs. accuracy.
*Observed training and validation loss across epochs to study underfitting and overfitting trends.
*Tried different convolution filter sizes (3×3, 5×5, etc.) to measure feature extraction quality.
*Applied dropout regularization to reduce overfitting and stabilize training.
*Compared performance differences between ANN and CNN, confirming CNN as the superior architecture for image-based tasks.


By following these guidelines, you can create a compelling LinkedIn "About Project" section that helps you stand out and attract the right opportunities. You can find more examples and tips on the LinkedIn Talent Blog. [1, 8]
'''

def write_about_project(details, about_project_rules, model = "gpt-4.1-mini"):
  #1. Creating  prompts
  system_prompt = f'''You are a professional project-description builder who specifically helps to write the 'About Project' section for resumes and LinkedIn.

                     Here's what you are expected to do:
                    1. Write a very enthusiastic, attention-grabbing project summary.
                    2. Keep it short, crisp, energetic, and engaging (6–8 lines).
                    3. Do not make it generic; highlight meaningful specifics from the user’s project details.
                    4. Personalize the description using the exact technical details provided by the user.
                    5. Return the output strictly in JSON using this format:

                    {{
                         "about": "...",
                         "highlights": ["...", "..."],
                        "tokens": <int>,
                        "tone": "energetic",
                        "sources": ["...","..."],
                        "confidence": <float between 0 and 1>
                    }}


                  Additionally, use the "about project rules" of do's and dont's to polish it further:
                  {about_project_rules}


                  '''

  user_prompt = details

  #2. Creating  message list
  messages = [
      {"role": "system", "content": system_prompt},
      {"role": "user", "content": user_prompt}
  ]


  #3. Call OpenAI
  response = openai.chat.completions.create(model=model, messages=messages, temperature=0.6)

  #4. return the results
  content = response.choices[0].message.content

  #5. Parse JSON safely
  try:
      data = json.loads(content)
  except:
      data = {"about": content, "highlights": [], "confidence": 0.5}

  return data

#Test:
project_details = '''This project focuses on building and comparing ANN and CNN models on the Fashion-MNIST dataset while experimenting with activation functions, hidden-layer depths, neuron configurations, CNN filter sizes, and dropout-based regularization.
                    It includes detailed analysis of loss trends across epochs and a performance comparison between ANN and CNN for image-recognition tasks.
                    Through systematic tuning and experimentation, the project demonstrates how architectural choices impact accuracy, convergence, and generalization.
                    This work strengthened my practical understanding of deep learning workflows, hyperparameter tuning, and model evaluation in computer vision.


            '''

write_about_project(project_details, LinkedIN_about_project_rules)

def validate_about_project(about_project_generated_json, about_project_original, model="gpt-4.1-mini"):
    system_prompt = f"""
                      You are a VERY STRICT project-description quality validator. You will receive:
                     1) the generated about-project JSON (field names: about, highlights, sources, tokens, tone, confidence)
                     2) the original project details used to generate the about-project.

                    Tasks:
                    - Verify factual consistency: every item in highlights/sources must be supported by the original project details or marked as hallucination.
                    - Verify length: "about" should be 6–10 lines (tolerate ±2).
                    - Verify tone and specificity (energetic, project-focused, not generic).
                    - Check that the project description correctly reflects tasks, techniques, tools, and insights from the user-provided project details.
                    - MAKE SURE YOU DO A VERY CRITICAL EVALUATION OF THE WRITE-UP.
                    - Produce a strict JSON response ONLY in this format:


                      {{
                      "decision": "go" or "no-go",
                      "score": <float 0..1>,
                      "reasons": ["..."],
                      "checks": {{
                        "length_ok": true/false,
                        "uses_specifics": true/false,
                        "no_hallucination": true/false
                      }}
                      """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": json.dumps({
            "generated": about_project_generated_json,
            "original_details": about_project_original
        })}
    ]

    #Call OpenAI
    resp = openai.chat.completions.create(model=model, messages=messages, temperature=0.0, max_tokens=400)
    content = resp.choices[0].message.content

    try:
        result = json.loads(content)
    except Exception:
        # fallback conservative answer
        result = {
            "decision": "no-go",
            "score": 0.0,
            "reasons": ["validator failed to parse model output"],
            "checks": {"length_ok": False, "uses_specifics": False, "no_hallucination": False},
            "suggested_about": ""
        }
    return result

#Unit Test
generated = write_about_project(project_details, LinkedIN_about_project_rules)
print(generated)
print(validate_about_project(generated, project_details))

generated

validate = validate_about_project(generated, project_details)
validate

type(validate)
validate["decision"]

#Testing...
project_details = "A deep learning project where I experimented with ANN and CNN models on the Fashion-MNIST dataset to study how activation functions, architecture choices, filters, and dropout impact image-classification performance."


count = 0
for i in range(5):
  count +=1
  generated = write_about_project(project_details, LinkedIN_about_project_rules)   # STEP 1: GENERATE ABOUT project
  validate = validate_about_project(generated, project_details)               # STEP 2: VALIDATE ABOUT project
  print(f"attempt: {count}, decision: {validate["decision"]}")
  if validate["decision"] == 'go':
    print(generated)
    print(validate)
    break

"""## Version 2"""

def write_about_project_v2(details, about_project_rules, model = "gpt-4.1-nano", feedback_history=None):


  if feedback_history:
      feedback_history_to_use = feedback_history[-3:]
  else:
      feedback_history_to_use = []

  # Convert to JSON string
  feedback_history_str = json.dumps(feedback_history_to_use, indent=2) if feedback_history_to_use else ""

  #1. Creating prompts
  system_prompt = f'''You are a professional project-description builder who specifically helps to write the 'About Project' section for resumes and LinkedIn.

                     Here's what you are expected to do:
                    1. Write a very enthusiastic, attention-grabbing project summary.
                    2. Keep it short, crisp, energetic, and engaging (6–8 lines).
                    3. Do not make it generic; highlight meaningful specifics from the user’s project details.
                    4. Personalize the description using the exact technical details provided by the user.
                    5. Return the output strictly in JSON using this format:

                        {{
                          "about": "...",
                          "highlights": ["...", "..."],
                          "tokens": <int>,
                          "tone": "energetic",
                          "sources": ["...","..."],
                          "confidence": <float between 0 and 1>
                        }}

                  Additionally, use the "about project" rules of do's and dont's to polish it further:
                  {about_project_rules}


                  Here are feedback of your previous work:
                  {feedback_history_str}
                  You should be able to learn from the feedback and improve your response.
                  '''

  user_prompt = details

  #2. Create your message list
  messages = [
      {"role": "system", "content": system_prompt},
      {"role": "user", "content": user_prompt}
  ]


  #3. Call OpenAI
  response = openai.chat.completions.create(model=model, messages=messages, temperature=0.4)

  #4. return the results
  content = response.choices[0].message.content

  #5. Parse JSON safely
  try:
      data = json.loads(content)
  except:
      data = {"about": content, "highlights": [], "confidence": 0.5}

  return data

def validate_about_project_v2(about_project_generated_json, about_project_original, about_project_rules, model="gpt-4.1-nano"):
    system_prompt = f"""
                      You are a resume-quality validator. You will receive:
                      1) the generated about-project JSON (field names: about, highlights, sources, tokens, tone, confidence)
                      2) the original user details used to generate the about-project.

                      Tasks:
                      - Verify factual consistency: every item in highlights/sources must be supported by original details or marked as hallucination.
                      - Verify length: about should be 6-10 lines (tolerate ±2).
                      - Verify tone and specificity (energetic, not generic).
                      - Produce a strict JSON response ONLY in this format:
                            {{
                            "decision": "go" or "no-go",
                            "score": <float 0..1>,
                            "reasons": ["..."],
                            "checks": {{
                              "length_ok": true/false,
                              "uses_specifics": true/false,
                              "no_hallucination": true/false
                            }}

                      - Be a sensitive evaluater and your score SHOULDN'T biased numbers like 0.55, 0.65, etc.
                      - TRY TO KEEP THE SCORES AS FLOATING NUMBERS BETWEEN 0 AND 1. (NEED NOT BE IN THE MULTIPLE OF 5 OR 10)
                      - Here is your SCORING WEIGHTS:
                              length = 20%  (should be short and crisp)
                              tone = 20% (should be energetic, vibrant, and not generic)
                              informative = 40% (should contains many specific information about the project and its experience; generic output = low score, specific output = high score)
                              overall construction = 20% (professionally build paragraph = high score, scattered = low score)



                  Make sure the generated texts are bounded by the following rules:
                  {about_project_rules}


                      """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": json.dumps({
            "generated": about_project_generated_json,
            "original_details": about_project_original
        })}
    ]

    resp = openai.chat.completions.create(model=model, messages=messages, temperature=0.0, max_tokens=400)
    content = resp.choices[0].message.content

    try:
        result = json.loads(content)
    except Exception:
        # fallback conservative answer
        result = {
            "decision": "no-go",
            "score": 0.0,
            "reasons": ["validator failed to parse model output"],
            "checks": {"length_ok": False, "uses_specifics": False, "no_hallucination": False},
        }
    return result

#Test
generated_response = write_about_project_v2(project_details, LinkedIN_about_project_rules, feedback_history=[])
validate_response =  validate_about_project_v2(generated_response, project_details, LinkedIN_about_project_rules)

print(generated_response)
print("\n\n")
print(validate_response)

"""## Create Feedback History"""

feedback = {}
feedback['generated text'] = generated_response['about']
feedback['validation'] = validate_response

feedback

#Feedback summarizer:
def summarize_feedback_about(feedback, model = "gpt-4.1-nano"):

  feedback_str = json.dumps(feedback, indent=2)

  #1. Create your prompts
  system_prompt = '''You are a feeback summarizer. You will receive a JSON structure with generated test and its validation.
                  Highlight the most critical part of the feedback. Do not keep it too short or too long.

                  You are required to summarize the feedback as follows:

                  - Attempt: "I built anomaly detection... (include full attempt)" → Decision: no-go. Feedback: too long, generic tone.
                  - Attempt: "AI engineer building smart printers..." → Decision: go. Feedback: crisp and specific.

                  hence make sure your output include the attempt, decision, feedback triplet.

                  '''

  user_prompt = feedback_str

  #2. Create your message list
  messages = [
      {"role": "system", "content": system_prompt},
      {"role": "user", "content": user_prompt}
  ]


  #3. Call OpenAI
  response = openai.chat.completions.create(model=model, messages=messages)

  #4. return the results
  response = response.choices[0].message.content
  return(response)

#Test
summarize_feedback_about(feedback)

history = []
history.append(summarize_feedback_about(feedback))
print(history)

"""## Feedback Loop"""

history

#Test
generated_response = write_about_project_v2(project_details, LinkedIN_about_project_rules, feedback_history=history)
validate_response =  validate_about_project_v2(generated_response, project_details, LinkedIN_about_project_rules)

print(generated_response)
print("\n")
print(validate_response)

feedback = {}
feedback['generated text'] = generated_response['about']
feedback['validation'] = validate_response
history.append(summarize_feedback_about(feedback))
print(history)

generated_response = write_about_project_v2(project_details, LinkedIN_about_project_rules, feedback_history=history)
validate_response =  validate_about_project_v2(generated_response, project_details, LinkedIN_about_project_rules)

print(generated_response)
print("\n")
print(validate_response)

#The feedback loop

project_details = '''This project focuses on building and comparing ANN and CNN models on the Fashion-MNIST dataset while experimenting with activation functions, hidden-layer depths, neuron configurations, CNN filter sizes, and dropout-based regularization.
                    It includes detailed analysis of loss trends across epochs and a performance comparison between ANN and CNN for image-recognition tasks.
                    Through systematic tuning and experimentation, the project demonstrates how architectural choices impact accuracy, convergence, and generalization.
                    This work strengthened my practical understanding of deep learning workflows, hyperparameter tuning, and model evaluation in computer vision.
            '''

response = []   #Store the detailed original history
history = []    #Store the history in a compressed form using the fb summarizer
for i in range(4):
  generated_response = write_about_project_v2(project_details, LinkedIN_about_project_rules, feedback_history=history)
  validate_response =  validate_about_project_v2(generated_response, project_details, LinkedIN_about_project_rules)
  feedback = {}
  feedback['generated text'] = generated_response['about']
  feedback['validation'] = validate_response
  response.append(feedback)
  history.append(summarize_feedback_about(feedback))
  print(validate_response["decision"], validate_response["score"])

history[0]

"""## Interactive Experience"""

project_details = '''This project focuses on building and comparing ANN and CNN models on the Fashion-MNIST dataset while experimenting with activation functions, hidden-layer depths, neuron configurations, CNN filter sizes, and dropout-based regularization.
                    It includes detailed analysis of loss trends across epochs and a performance comparison between ANN and CNN for image-recognition tasks.
                    Through systematic tuning and experimentation, the project demonstrates how architectural choices impact accuracy, convergence, and generalization.
                    This work strengthened my practical understanding of deep learning workflows, hyperparameter tuning, and model evaluation in computer vision.
            '''

response = []
history = []
score = 0
count = 0
for i in range(10):
  count += 1
  generated_response = write_about_project_v2(project_details, LinkedIN_about_project_rules, feedback_history=history)
  validate_response =  validate_about_project_v2(generated_response, project_details, LinkedIN_about_project_rules)
  feedback = {}
  feedback['generated text'] = generated_response['about']
  feedback['validation'] = validate_response
  response.append(feedback)
  history.append(summarize_feedback_about(feedback))
  print(validate_response["decision"], validate_response["score"])

  if validate_response["score"] > score:
    score = validate_response["score"]
    best_response = generated_response['about']
    best_response_validation = validate_response

  if validate_response["decision"] == "go":
    best_response = generated_response['about']
    print(best_response)
    break

  if count >= 4 and count%2 == 0:
    #Ask user to enter more details
    print("The framework is currently generating very generic response. Enter more specific details about your work, experience, projects, or yourself")
    print(f"This is the best response so far:\n{best_response}")
    print("Type 'keep' to keep the best response and terminate the loop")
    more_details = input("Enter further details here: ")
    if more_details.lower() == "keep":
      break
    elif more_details.lower() == "quit":
      break
    else:
      project_details += more_details

response[-2:]

display(Markdown(best_response))

#Version 3 - Validation (Add validation history)

def validate_about_project_v3(about_project_generated_json, about_project_original, about_project_rules, validation_history=None, model="gpt-4.1-nano"):

    # Convert to JSON string (pretty format) only if non-empty
    feedback_history_str = json.dumps(validation_history, indent=2) if validation_history else ""

    system_prompt = f"""
                      You are a resume-quality validator. You will receive:
                      1) the generated about-project JSON (field names: about, highlights, sources, tokens, tone, confidence)
                      2) the original user details used to generate the about-me.

                      Tasks:
                      - Verify factual consistency: every item in highlights/sources must be supported by original details or marked as hallucination.
                      - Verify length: about should be 6-10 lines (tolerate ±2).
                      - Verify tone and specificity (energetic, not generic).
                      - Produce a strict JSON response ONLY in this format:
                            {{
                            "decision": "go" or "no-go",
                            "score": <float 0..1>,
                            "reasons": ["..."],
                            "checks": {{
                              "length_ok": true/false,
                              "uses_specifics": true/false,
                              "no_hallucination": true/false
                            }}
                      - Be a sensitive evaluater and your score SHOULDN'T biased numbers like 0.55, 0.65, etc.
                      - TRY TO KEEP THE SCORES AS FLOATING NUMBERS BETWEEN 0 AND 1. (NEED NOT BE IN THE MULTIPLE OF 5 OR 10)
                      - Here is your SCORING WEIGHTS:
                              length = 20%  (should be short and crisp)
                              tone = 20% (should be energetic, vibrant, and not generic)
                              informative = 40% (should contains many specific information about the person and its experience; generic output = low score, specific output = high score)
                              overall construction = 20% (professionally build paragraph = high score, scattered = low score)
                      - Here is your validation feedback for the last two evaluation {feedback_history_str}.
                      - The score you produce for the current generated response should be relative to the above feedback and should not be done independent of the above feedback.
                        Which means if the current generated test is better than {feedback_history_str} then you should give a higher score, else give lower score.


                  Make sure the generated texts are bounded by the following rules:
                  {about_project_rules}


                      """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": json.dumps({
            "generated": about_project_generated_json,
            "original_details": about_project_original
        })}
    ]

    resp = openai.chat.completions.create(model=model, messages=messages, temperature=0.0, max_tokens=400)
    content = resp.choices[0].message.content

    try:
        result = json.loads(content)
    except Exception:
        # fallback conservative answer
        result = {
            "decision": "no-go",
            "score": 0.0,
            "reasons": ["validator failed to parse model output"],
            "checks": {"length_ok": False, "uses_specifics": False, "no_hallucination": False},
        }
    return result

#Test
project_details = '''This project focuses on building and comparing ANN and CNN models on the Fashion-MNIST dataset while experimenting with activation functions, hidden-layer depths, neuron configurations, CNN filter sizes, and dropout-based regularization.
                    It includes detailed analysis of loss trends across epochs and a performance comparison between ANN and CNN for image-recognition tasks.
                    Through systematic tuning and experimentation, the project demonstrates how architectural choices impact accuracy, convergence, and generalization.
                    This work strengthened my practical understanding of deep learning workflows, hyperparameter tuning, and model evaluation in computer vision.
            '''

response = []
history = []
score = 0
count = 0
best_response_validation = None

for i in range(10):
  count += 1
  generated_response = write_about_project_v2(project_details, LinkedIN_about_project_rules, feedback_history=history)
  validate_response =  validate_about_project_v3(generated_response, project_details, LinkedIN_about_project_rules, validation_history=response[-2:])
  feedback = {}
  feedback['generated text'] = generated_response['about']
  feedback['validation'] = validate_response
  response.append(feedback)
  history.append(summarize_feedback_about(feedback))
  print(validate_response["decision"], validate_response["score"])

  if validate_response["score"] > score:
    score = validate_response["score"]
    best_response = generated_response['about']
    best_response_validation = validate_response

  if validate_response["decision"] == "go":
    best_response = generated_response['about']
    print(best_response)
    break

  if count >= 4 and count%2 == 0:
    #Ask user to enter more details
    print("The framework is currently generating very generic response. Enter more specific details about your work, experience, projects, or yourself")
    print(f"This is the best response so far:\n{best_response}")
    print("Type 'keep' to keep the best response and terminate the loop")
    more_details = input("Enter further details here: ")
    if more_details.lower() == "keep":
      break
    else:
      my_details += more_details

display(Markdown(best_response))